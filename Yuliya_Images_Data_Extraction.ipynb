{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Ko2gVEMEW2nJ",
        "4MGjCLrwV0dF",
        "-FAxGFCWV20f"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rafaellagp/Images_Data_Extraction/blob/main/Yuliya_Images_Data_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Yuliya_Images_Data_Extraction"
      ],
      "metadata": {
        "id": "Vzj02f12SyEU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Introduction*\n",
        "\n",
        "This project aims to classify images based on their content, extract text from images, classify text into categories, and generate emotions associated with the text. It uses OpenAI's CLIP model for zero-shot image classification, EasyOCR for text extraction, langdetect for language detection, and GPT-3 for emotion generation.\n",
        "\n",
        "*Installation*\n",
        "\n",
        "Before running the script, ensure that the necessary libraries are installed by running the first cell in the notebook. The code checks the version of CUDA installed on the system and sets the PyTorch version suffix accordingly to ensure GPU acceleration. Finally, it installs PyTorch and other necessary libraries using the PyTorch stable download link and clip-by-openai library.\n",
        "\n",
        "*Functions*\n",
        "\n",
        "The notebook includes several functions for image classification, text extraction and translation, text classification, and emotion generation. Each function takes specific inputs and returns specific outputs.\n",
        "\n",
        "*Usage*\n",
        "\n",
        "To use the script, input the desired images into the \"images\" folder, set the necessary parameters in the \"Set parameters\" section, and run the final function \"just_do_it\". The output will be a CSV file in the \"csv_result\" folder that contains the image filename, image category probabilities, extracted text, text category probabilities, text category key words, and associated emotions.\n",
        "\n",
        "*Parameters*\n",
        "\n",
        "The \"Set parameters\" section includes variables for the categories, category words dictionary, and tokenize category detection. Modify these variables to fit the desired categories and words related to each category.\n",
        "\n",
        "*Credits*\n",
        "\n",
        "This project was developed by Alexis Vandriessche, Piero Rucci, Rafaella Porto and Pierre Warnier as part of Becode Ai Bootcamp. The code was developed using Python and various open-source libraries, including PyTorch, CLIP, EasyOCR, langdetect, spaCy, deep_translator, and OpenAI's GPT-3."
      ],
      "metadata": {
        "id": "y9Nkn9WC_bLN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Libraries Installation and imports\n",
        "\n",
        "The fist step is to install all the libraries necessaries to run the following functions. Then, it checks the version of CUDA installed on the system and sets a PyTorch version suffix accordingly. This suffix specifies the version of CUDA that the PyTorch library was built with and ensures that PyTorch can use GPU acceleration. The code checks the CUDA version and sets the PyTorch version suffix to \"+cu100\" for CUDA 10.0, \"+cu101\" for CUDA 10.1, an empty string for CUDA 10.2, and \"+cu110\" for any other version. Finally, it installs PyTorch and other necessary libraries using the PyTorch stable download link and clip-by-openai library."
      ],
      "metadata": {
        "id": "P1oiawm7S2BT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install easyocr\n",
        "! pip install langdetect\n",
        "! pip install deep-translator\n",
        "! pip install spacy\n",
        "! python -m spacy download en_core_web_md\n",
        "! pip install classy-classification\n",
        "! pip install textblob\n",
        "! pip install openai\n",
        "! pip install torch\n",
        "! pip install clip-by-openai"
      ],
      "metadata": {
        "id": "0g2yacHoS4gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import locale\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import easyocr\n",
        "from langdetect import detect\n",
        "from deep_translator import GoogleTranslator\n",
        "import spacy\n",
        "import classy_classification\n",
        "from textblob import TextBlob\n",
        "import openai\n",
        "from PIL import Image\n",
        "import torch\n",
        "import clip"
      ],
      "metadata": {
        "id": "VeF58DX3S53b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Image classification function: Clip zero-shot image classification\n",
        "\n",
        "[CLIP](https://openai.com/blog/clip/) (Contrastive Language-Image Pre-Training) is a neural network model developed by OpenAI that performs zero-shot classification on images. It is called \"zero-shot\" because it can classify images into a large number of categories without being trained on any of them explicitly. CLIP uses a combination of both natural language processing (NLP) and computer vision (CV) techniques to perform classification. Specifically, it is trained on a large dataset of images and their corresponding natural language descriptions, which it uses to learn how to associate different visual features with different words and concepts. At inference time, CLIP can take in any image and a textual description of a category (e.g. \"food\", \"energy\", \"transport\", etc.), and use its learned associations to predict the likelihood that the image belongs to that category. Because the model is trained on a large number of different categories, it can perform classification on a wide range of tasks, including those it has never seen before. CLIP has achieved state-of-the-art results on several image classification benchmarks and has been applied to various tasks such as image retrieval, visual question answering, and image captioning.\n",
        "\n",
        "The **get_image_category** function takes as input a path to an image, a list of categories, and a pre-trained CLIP model by OpenAI for zero-shot classification. It returns a dictionary that contains the probabilities of the image belonging to each of the specified categories. The function loads the image and encodes the image features using the CLIP model and gets the logits for each category using the model function. The logits are then converted to probabilities using softmax and returned in the form of a dictionary with the category names as keys and probabilities as values."
      ],
      "metadata": {
        "id": "Ko2gVEMEW2nJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_category(path_image,categories,tokenize_category_detection):\n",
        "  image = transform(Image.open(path_image)).unsqueeze(0).to(device)\n",
        "  with torch.no_grad():\n",
        "      image_features = model.encode_image(image)\n",
        "      logits_per_image, logits_per_text = model(image, tokenize_category_detection)\n",
        "      probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "  return {f\"image category {category} probability\" : prob for category,prob in zip(categories,probs[0])}"
      ],
      "metadata": {
        "id": "HIuQEltSW3Aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. OCR Functions: Text extraction and translation\n",
        "\n",
        "The next step aims to extract text from the images, detect the language of the extracted text, and translate the text to English. The **get_text** function reads images and extracts the text from them using the EasyOCR package. The **detect_lang** function detects the language of the extracted text using the langdetect package. Finally, the **translate_text_to_english** function will translate the extracted text to English using the deep_translator package."
      ],
      "metadata": {
        "id": "4MGjCLrwV0dF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text(images_path,image_name):\n",
        "  result = reader.readtext(path_images+image_name)\n",
        "  return (\" \".join(data[1] for data in result)).lower()\n",
        "\n",
        "def detect_lang(original_text): #can be one line in the main function\n",
        "  return detect(original_text)\n",
        "\n",
        "def translate_text_to_english(original_text):\n",
        "  return GoogleTranslator(source='auto', target='en').translate(original_text)"
      ],
      "metadata": {
        "id": "MeGYkVy-V2Wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. NLP Functions: Text classification - Categories and Emotions\n",
        "\n",
        "The main objective of the next step is to perform natural language processing (NLP) on an English text and extract its category and key words related to that category. The function **get_text_category()** takes the English text as input, processes it with spaCy, and returns a dictionary of the probability of the text belonging to different categories. The function **get_text_category_key_word()** takes the English text and a reference dictionary of key words for each category as input. It processes the text with spaCy, identifies the main words (verbs, nouns, and proper nouns), and finds the key words for each category by checking if the main words are in the reference dictionary. It returns a dictionary of the key words found for each category.\n",
        "\n",
        "The next step is to generate emotions that are associated with a given text. The function **get_text_emotion** takes an English text (extrated from the inputed images - original or translated) and sends it as a prompt to OpenAI's GPT-3 language model (using the \"text-davinci-003\" engine). The prompt asks the model to provide a list of one or more emotions (\"Altruistic\", \"Biospheric\", \"Hedonic\", \"Egotistic\", \"Undefined\") that match the text, in Python list format. The response from the GPT-3 model is then processed to extract the list of emotions and returned by the function."
      ],
      "metadata": {
        "id": "-FAxGFCWV20f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Text Categories Analysis Functions"
      ],
      "metadata": {
        "id": "UiflIaMQWKvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text_category(english_text):\n",
        "  text_category_prob_dict = nlp(english_text)._.cats\n",
        "  return {f\"text category {key} probabilty\" : value for key, value in text_category_prob_dict.items()}\n",
        "\n",
        "def get_text_category_key_word(english_text,reference_dict):\n",
        "  tokens = nlp(english_text)\n",
        "  main_words = []\n",
        "  text_categories_keywords = {}\n",
        "  for token in tokens :\n",
        "    if token.pos_ == \"VERB\" or token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\" :\n",
        "      main_words.append(token.lemma_)\n",
        "  main_words = list(set(main_words))\n",
        "  for category, words_list in reference_dict.items():\n",
        "    category_key_words_find = []\n",
        "    for word in main_words :\n",
        "      if word in words_list :\n",
        "        category_key_words_find.append(word)\n",
        "    text_categories_keywords[f\"text {category} key words\"] = category_key_words_find\n",
        "  return text_categories_keywords"
      ],
      "metadata": {
        "id": "3pgE-FejWDfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Text Emotions Analysis Functions"
      ],
      "metadata": {
        "id": "mNwS8T8iWFUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text_polarity(english_text):\n",
        "  return TextBlob(english_text).polarity\n",
        "\n",
        "def get_text_emotion(english_text,emotions):\n",
        "  emotions_string = \", \".join(emotion for emotion in emotions)\n",
        "  response = openai.Completion.create(engine=\"text-davinci-003\",\n",
        "                                      prompt=f\"With forgetting the previous queries, can you say me which values [{emotions_string}] match the following text : {english_text}, you can only answer in a python list format with one or more of the four values.\",\n",
        "                                      temperature=0.5)\n",
        "  return {emotion : True if emotion in response[\"choices\"][0][\"text\"].replace(\"\\n\", \"\") else False for emotion in emotions}"
      ],
      "metadata": {
        "id": "qN4ATy5tWSij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Main Functions: Agregate previous functions"
      ],
      "metadata": {
        "id": "5p0iwlfh3lf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def just_do_it(path_images,path_csv_result,categories,tokenize_category_detection,categories_words_dict,emotions):\n",
        "  \n",
        "  images_names_list = os.listdir(path_images)\n",
        "  images_extracted_data_list = []\n",
        "\n",
        "  for image_id in images_names_list :\n",
        "    image_data_dict = {}\n",
        "    image_data_dict[\"image_id\"] = image_id\n",
        "    image_data_dict.update(get_image_category(path_images+image_id,categories,tokenize_category_detection))\n",
        "    image_data_dict[\"text\"] = get_text(path_images,image_id)\n",
        "    if image_data_dict[\"text\"] != \"\" :\n",
        "      image_data_dict[\"text language\"] = detect_lang(image_data_dict[\"text\"])\n",
        "      image_data_dict[\"text translate english\"] = translate_text_to_english(image_data_dict[\"text\"])\n",
        "      image_data_dict.update(get_text_category(image_data_dict[\"text translate english\"]))\n",
        "      image_data_dict.update(get_text_category_key_word(image_data_dict[\"text translate english\"],categories_words_dict))\n",
        "      image_data_dict[\"text polarity\"] = get_text_polarity(image_data_dict[\"text translate english\"])\n",
        "      image_data_dict.update(get_text_emotion(image_data_dict[\"text translate english\"],emotions))\n",
        "      images_extracted_data_list.append(image_data_dict)\n",
        "    else :\n",
        "      image_data_dict[\"text language\"] = \"\"\n",
        "      image_data_dict[\"text translate english\"] = \"\"\n",
        "      image_data_dict.update({f\"text category {key} probabilty\" : \"\" for key in categories_words_dict.keys()})\n",
        "      image_data_dict.update({f\"text {key} key words\" : \"\" for key in categories_words_dict.keys()})\n",
        "      image_data_dict[\"text polarity\"] = \"\"\n",
        "      image_data_dict.update({emotion : \"\" for emotion in emotions})\n",
        "      images_extracted_data_list.append(image_data_dict)\n",
        "\n",
        "  dataframe_image_data = pd.DataFrame(images_extracted_data_list)\n",
        "  csv_name = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  dataframe_image_data.to_csv(path_csv_result+csv_name+\".csv\")\n",
        "\n",
        "  return None"
      ],
      "metadata": {
        "id": "X_3-49Ki3lks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Script Basis: set necessary parameters\n",
        "\n",
        "After setting all the necessary functions to process and classify images, the next step is to set the parameters necessary for the tasks. \n",
        "\n",
        "The **categories** variable is a list of strings containing the names of the three categories.\n",
        "\n",
        "The **categories_words_dict** dictionary contains a mapping of each category to a list of words related to that category. For example, the Energy category is mapped to a list of words such as \"energy,\" \"photovoltaic,\" \"fuel,\" and \"renewable,\" among others.\n",
        "\n",
        "The **tokenize_category_detection** variable tokenizes a list of strings, where each string related to each category. This will be used as the input to the model."
      ],
      "metadata": {
        "id": "CgLFo8HOlFSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categories = [\"Energy\", \"Transport\", \"Food\"]\n",
        "categories_words_dict ={\"Energy\" : [\"energy\", \"photovoltaic\", \"fuel\", \"biofuel\", \"green\", \"transition\", \"fossil\", \"electricity\", \"heating\", \"coal\", \"oil\", \"gas\", \"nuclear\", \"battery\", \"renewable\", \"cell\", \"solar\", \"wind\", \"turbine\"],\n",
        "                  \"Transport\" : [\"transport\", \"train\", \"bike\", \"car\", \"plane\", \"boat\", \"drive\", \"cyclist\", \"bike path\", \"fly\", \"rail\", \"sncf\", \"sncb\", \"suv\", \"diesel\", \"highway\", \"tesla\", \"oil industry\", \"petroleum\", \"stopsuv\", \"scooter\", \"walking\", \"rollerblading\", \"bus\", \"airport\"],\n",
        "                  \"Food\" : [\"pesticide\", \"agriculture\", \"drought\", \"overfishing\", \"CAP\", \"permaculture\", \"compost\", \"meat\", \"vegetable\", \"fruit\", \"garden\", \"organic product\", \"food\", \"eat\", \"feed\", \"vegetarian\", \"vegan\"]}\n",
        "categories_examples_dict ={\"Energy\" : [\"energy\", \"photovoltaic\", \"fuel\", \"biofuel\", \"green\", \"transition\", \"fossil\", \"electricity\", \"heating\", \"coal\", \"oil\", \"gas\", \"nuclear\", \"battery\", \"renewable\", \"cell\", \"solar\", \"wind\", \"turbine\"],\n",
        "                  \"Transport\" : [\"transport\", \"train\", \"bike\", \"car\", \"plane\", \"boat\", \"drive\", \"cyclist\", \"bike path\", \"fly\", \"rail\", \"sncf\", \"sncb\", \"suv\", \"diesel\", \"highway\", \"tesla\", \"oil industry\", \"petroleum\", \"stopsuv\", \"scooter\", \"walking\", \"rollerblading\", \"bus\", \"airport\"],\n",
        "                  \"Food\" : [\"pesticide\", \"agriculture\", \"drought\", \"overfishing\", \"CAP\", \"permaculture\", \"compost\", \"meat\", \"vegetable\", \"fruit\", \"garden\", \"organic product\", \"food\", \"eat\", \"feed\", \"vegetarian\", \"vegan\"]}\n",
        "emotions = [\"Altruistic\", \"Biospheric\", \"Hedonic\", \"Egotistic\", \"Undefined\"]\n",
        "\n",
        "locale.getpreferredencoding = \"UTF-8\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, transform = clip.load(\"ViT-B/32\", device=device)\n",
        "tokenize_category_detection = clip.tokenize([f\"An example of a picture related to {category}\" for category in categories]).to(device)\n",
        "reader = easyocr.Reader([\"en\"],gpu = True)\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "nlp.add_pipe(\"text_categorizer\", config={\"data\": categories_examples_dict,\"model\": \"spacy\"})\n",
        "openai.api_key = \"openai.api_key""
      ],
      "metadata": {
        "id": "9nAAtCY_TGD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Main Script\n",
        "\n",
        "The final step is to create the folder 'image/' and manualy input the images that will be process. After run the final function [**just_do_it**](https://www.youtube.com/watch?v=5-sfG8BV8wU) the result will be the .csv into the folder 'csv_result/'. "
      ],
      "metadata": {
        "id": "yUkWnkC3Wcwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_images = \"/content/images/\"\n",
        "path_csv_result = \"/content/csv_result/\"\n",
        "\n",
        "just_do_it(path_images,path_csv_result,categories,tokenize_category_detection,categories_words_dict,emotions)"
      ],
      "metadata": {
        "id": "44_9xDUPZjLZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
